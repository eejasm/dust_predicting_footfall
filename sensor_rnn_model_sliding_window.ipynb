{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d6ab199-b664-422f-b148-8fd3b821a929",
   "metadata": {},
   "source": [
    "# Testing the window size on model predictability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c478a-129d-4645-af88-eedb8b83fc1e",
   "metadata": {},
   "source": [
    "This notebook contains the same code as sensor_rnn_model. The difference comes at the end where multiple different training runs are performed to test how changing the window size affects the skill of the model's footfall predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3cbe6a2-1366-47c2-8726-9eaae4bf33c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, mean_absolute_error, mean_absolute_percentage_error, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import sklearn\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "#!pip install seaborn\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e8dce-b37b-4981-9a67-2f2f1df89c5c",
   "metadata": {},
   "source": [
    "## Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "141dd8c8-f3af-4eef-b19b-7f85fe89da72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_x_y_data(input_csv):\n",
    "    # Read in formatted data\n",
    "    data = pd.read_csv(input_csv, index_col = False)\n",
    "    data = data.fillna(0)\n",
    "    \n",
    "    ### Delete unneeded columns - we currently include data from all sensors (even incomplete ones)\n",
    "    sensor_ids = data['sensor_id']\n",
    "    #data = data.drop(['sensor_id'],axis=1) # don't want this included\n",
    "    # Get rid of columns in which none of the sensors have a value\n",
    "    for column in data.columns:\n",
    "        if np.nanmax(data[column]) ==0:\n",
    "            del data[column]\n",
    "            \n",
    "    # Filter columns using the regex pattern in function input\n",
    "    regex_pattern = 'buildings$|street_inf$|landmarks$'\n",
    "    data = data[data.columns.drop(list(data.filter(regex=regex_pattern)))].copy()\n",
    "    \n",
    "    ### Add a random variable (to compare performance of other variables against)\n",
    "    rng = np.random.RandomState(seed=42)\n",
    "    data['random'] = np.random.random(size=len(data))\n",
    "    data[\"random_cat\"] = rng.randint(3, size=data.shape[0])\n",
    "    \n",
    "    ## Prepare data for modelling \n",
    "    ### Split into predictor/predictand variables\n",
    "    Xfull = data.drop(['hourly_counts'], axis =1)\n",
    "    Yfull = data['hourly_counts'].values\n",
    "       \n",
    "    ### Store the (non Sin/Cos) time columns and then remove them (Need them later to segment the results by hour of the day)\n",
    "    data_time_columns = Xfull[['day_of_month_num', 'time', 'weekday_num', 'time_of_day', 'datetime']]\n",
    "    #Xfull = Xfull.drop(['day_of_month_num', 'time', 'weekday_num', 'time_of_day','datetime', 'month_num'],axis=1)\n",
    "    Xfull = Xfull.drop(['day_of_month_num', 'time', 'weekday_num', 'time_of_day', 'month_num'],axis=1)\n",
    "    return Xfull, Yfull, data_time_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19141b78-80e0-4213-b157-1ec80e5a91b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data normalizing function\n",
    "def normalize(df, target_column):\n",
    "    \n",
    "    result = df.copy()\n",
    "    \n",
    "    for feature_name in df.columns:\n",
    "        \n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        \n",
    "        if feature_name == 'footfall':\n",
    "            result['footfall_norm'] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "            \n",
    "        else:\n",
    "            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    \n",
    "    cols = list(result.columns)\n",
    "    column_list = cols[:-2] + cols[-1:] + cols[-2:-1]\n",
    "    result = result[column_list]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78429c73-fbb4-4295-8b82-76fbf8285283",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in the raw data - see .README file for info on the raw data\n",
    "buffer_size_m = 400\n",
    "input_csv =\"/lustre_scratch/eejasm/DUST/formatted_data_for_modelling_allsensors_{}.csv\".format(buffer_size_m)\n",
    "\n",
    "X_data, Y_data, data_time_columns = prepare_x_y_data(input_csv)\n",
    "\n",
    "X_data = X_data.iloc[:2198889]\n",
    "Y_data = Y_data[:2198889]\n",
    "data_time_columns = data_time_columns.iloc[:2198889]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d3beacf-e370-4a7b-bf46-76d19c61f485",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "51\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "# Find the sensors that have continuous data throughout the time period (no hourly gaps)\n",
    "sensor_and_date = X_data.iloc[:,:2]\n",
    "for _id in set(sorted(sensor_and_date['sensor_id'])):\n",
    "    \n",
    "    datetimes = list(sensor_and_date.loc[sensor_and_date['sensor_id']== _id, 'datetime'])\n",
    "    empty_list = []\n",
    "    \n",
    "    for i in range(len(datetimes)-1):\n",
    "        if datetime.strptime(datetimes[i+1], \"%Y-%m-%d %H:%M:%S\") != datetime.strptime(datetimes[i], \"%Y-%m-%d %H:%M:%S\") + timedelta(hours=1):\n",
    "            empty_list.append(1)\n",
    "            break\n",
    "    \n",
    "    if sum(empty_list) < 1:\n",
    "        print(_id)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48632167-ed63-4940-963e-453d117298a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose one sensor to train the model on\n",
    "sensor_id = 54\n",
    "X_data = X_data.loc[X_data['sensor_id'] == sensor_id]\n",
    "Y_data = Y_data[X_data.loc[X_data['sensor_id'] == sensor_id].index]\n",
    "data_time_columns = data_time_columns.iloc[X_data.loc[X_data['sensor_id'] == sensor_id].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51bf475c-96ba-474c-b918-afe912ce0b63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a copy of the data to combine X and Y\n",
    "df = X_data.copy()\n",
    "df['footfall'] = Y_data\n",
    "\n",
    "# extraxt max values for normalization\n",
    "footfall_max = df['footfall'].max()\n",
    "footfall_min = df['footfall'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e101d44-084b-43ff-bb95-706e53e6c501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split into train, val and test sets\n",
    "n=len(df)\n",
    "\n",
    "train_df = df[0:int(n*0.7)]\n",
    "val_df = df[int(n*0.7):int(n*0.9)]\n",
    "test_df = df[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6953df22-fd7a-4aba-a539-70f688ea097c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose the features to include as predictors\n",
    "features = ['footfall']\n",
    "\n",
    "train_df = train_df.loc[:, features]\n",
    "val_df = val_df.loc[:, features]\n",
    "test_df = test_df.loc[:, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24b3737a-00a8-4f44-a93a-b7b9548b1a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalize and drop na fields\n",
    "train_df = normalize(train_df, 'footfall')\n",
    "val_df = normalize(val_df, 'footfall')\n",
    "test_df = normalize(test_df, 'footfall')\n",
    "\n",
    "train_df = train_df.dropna(axis='columns')\n",
    "val_df = val_df.dropna(axis='columns')\n",
    "test_df = test_df.dropna(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d590b640-e695-4fda-88ff-183131e1fba3",
   "metadata": {},
   "source": [
    "## Time series forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5f25898-97bf-4a72-aa6c-9ff85be8f51c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a window generator that generates windows of data for input into model training\n",
    "# also has a plotting routine to check the results\n",
    "\n",
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=None):\n",
    "        \n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "    \n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        \n",
    "        self.column_indices = {name: i for i, name in\n",
    "                            enumerate(train_df.columns)}\n",
    "    \n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "    \n",
    "        self.total_window_size = input_width + shift\n",
    "    \n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "    \n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "    \n",
    "    def split_window(self, features):\n",
    "        \n",
    "        inputs = features[:, self.input_slice, -1:]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        #print(inputs.shape)\n",
    "        #inputs[:,:,-1] = (inputs[:,:,-1] - footfall_min) / (footfall_max - footfall_min)\n",
    "\n",
    "        \n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns],axis=-1)\n",
    "    \n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "    \n",
    "        return inputs, labels\n",
    "    \n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=32,)\n",
    "        ds = ds.map(self.split_window)\n",
    "        return ds\n",
    "    \n",
    "    # a plot function to view a few examples of how the footfall prediction compares to the observed value\n",
    "    def plot(self, model=None, plot_col='footfall', max_subplots=10):\n",
    "    \n",
    "        inputs, labels = self.example\n",
    "        plt.figure(figsize=(12, 18))\n",
    "        plot_col_index = self.column_indices[plot_col]\n",
    "        max_n = min(max_subplots, len(inputs))\n",
    "        print(max_n)\n",
    "        for n in range(max_n):\n",
    "            plt.subplot(max_n, 1, n+1)\n",
    "            plt.ylabel(f'{plot_col} [normed]')\n",
    "            plt.plot(self.input_indices, inputs[n, :, plot_col_index-1],\n",
    "                     label='Inputs', marker='.', zorder=-10)\n",
    "        \n",
    "            if self.label_columns:\n",
    "                label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "            else:\n",
    "                label_col_index = plot_col_index\n",
    "        \n",
    "            if label_col_index is None:\n",
    "                continue\n",
    "        \n",
    "            plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                        edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "            if model is not None:\n",
    "                predictions = model(inputs)\n",
    "                plt.scatter(self.label_indices, predictions[n,label_col_index],\n",
    "                          marker='X', edgecolors='k', label='Predictions',\n",
    "                          c='#ff7f0e', s=64)\n",
    "        \n",
    "            if n == 0:\n",
    "                plt.legend()\n",
    "        \n",
    "        plt.xlabel('Time [h]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77560eb1-aba1-4526-8bb8-13f15ca3f58a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set epochs and function for compile/training the model\n",
    "MAX_EPOCHS = 200\n",
    "\n",
    "def compile_and_fit(model, window, patience=3):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                      patience=patience,\n",
    "                                                      mode='min')\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                        validation_data=window.val,\n",
    "                        callbacks=[early_stopping],\n",
    "                       verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e3d8a9a-3e41-4274-9c86-9f5700102dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup window with required parameters\n",
    "w2 = WindowGenerator(input_width=6, label_width=1, shift=1, label_columns=['footfall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da1560d0-bcf2-4c54-af57-9343d21b7818",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 11:38:11.813044: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-26 11:38:12.961723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 208 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB-LS, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2024-01-26 11:38:12.964103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30436 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB-LS, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "2024-01-26 11:38:12.978799: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 208.88M (219021312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
     ]
    }
   ],
   "source": [
    "example_window = tf.stack([np.array(train_df[:w2.total_window_size]),\n",
    "                           np.array(train_df[100:100+w2.total_window_size]),\n",
    "                           np.array(train_df[200:200+w2.total_window_size])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "908776cd-0208-44b4-b268-7350d6ba36cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All shapes are: (batch, time, features)\n",
      "Window shape: (3, 7, 2)\n",
      "Inputs shape: (3, 6, 1)\n",
      "Labels shape: (3, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# set to check output looks correct\n",
    "example_inputs, example_labels = w2.split_window(example_window)\n",
    "\n",
    "print('All shapes are: (batch, time, features)')\n",
    "print(f'Window shape: {example_window.shape}')\n",
    "print(f'Inputs shape: {example_inputs.shape}')\n",
    "print(f'Labels shape: {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da2c7447-6e3c-4910-b611-6c3ca55f67fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the WindowGenerator object holds training, validation, and test data. \n",
    "# Here add properties for accessing them as tf.data.Datasets using the make_dataset method you defined earlier\n",
    "\n",
    "@property\n",
    "def train(self):\n",
    "    return self.make_dataset(self.train_df)\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "    return self.make_dataset(self.val_df)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "    return self.make_dataset(self.test_df)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "    result = getattr(self, '_example', None)\n",
    "    if result is None:\n",
    "      # No example batch was found, so get one from the `.train` dataset\n",
    "      result = next(iter(self.train))\n",
    "      # And cache it for next time\n",
    "      self._example = result\n",
    "    return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.val = val\n",
    "WindowGenerator.test = test\n",
    "WindowGenerator.example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4df2a61e-007f-4574-9473-a4ec0abd5ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape (batch, time, features): (32, 6, 1)\n",
      "Labels shape (batch, time, features): (32, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# check each batch before going into training\n",
    "for example_inputs, example_labels in w2.train.take(1):\n",
    "    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "    print(f'Labels shape (batch, time, features): {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cb3c65f-c7cf-4fd4-b419-f59642694fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup the LSTM model\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f93621-0f07-4d94-bf66-fa7369b8165a",
   "metadata": {},
   "source": [
    "## Loop through different sliding windows and save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "401f9272-55a9-47a2-9358-86dfa3810bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (32, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 11:38:14.229369: E tensorflow/stream_executor/cuda/cuda_dnn.cc:389] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n",
      "2024-01-26 11:38:14.229430: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_rnn_ops.cc:1553 : UNKNOWN: Fail to find the dnn implementation.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Exception encountered when calling layer \"lstm_1\" (type LSTM).\n\nFail to find the dnn implementation. [Op:CudnnRNN]\n\nCall arguments received by layer \"lstm_1\" (type LSTM):\n  • inputs=tf.Tensor(shape=(32, 1, 1), dtype=float32)\n  • mask=None\n  • training=None\n  • initial_state=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Shape [batch, time, features] => [batch, time, lstm_units]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLSTM(\u001b[38;5;241m32\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Shape => [batch, time, features]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m ])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, w2\u001b[38;5;241m.\u001b[39mexample[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mlstm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m val_performance \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     16\u001b[0m performance \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Exception encountered when calling layer \"lstm_1\" (type LSTM).\n\nFail to find the dnn implementation. [Op:CudnnRNN]\n\nCall arguments received by layer \"lstm_1\" (type LSTM):\n  • inputs=tf.Tensor(shape=(32, 1, 1), dtype=float32)\n  • mask=None\n  • training=None\n  • initial_state=None"
     ]
    }
   ],
   "source": [
    "for window_width in range(1,25):\n",
    "    \n",
    "    w2 = WindowGenerator(input_width=window_width, label_width=1, shift=1, label_columns=['footfall'])\n",
    "    \n",
    "    lstm_model = tf.keras.models.Sequential([\n",
    "        # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "        tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "        # Shape => [batch, time, features]\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "    ])\n",
    "    \n",
    "    print('Input shape:', w2.example[0].shape)\n",
    "    print('Output shape:', lstm_model(w2.example[0]).shape)\n",
    "    \n",
    "    val_performance = {}\n",
    "    performance = {}\n",
    "    \n",
    "    history = compile_and_fit(lstm_model, w2)\n",
    "    \n",
    "    val_performance['LSTM'] = lstm_model.evaluate(w2.val)[1]\n",
    "    performance['LSTM'] = lstm_model.evaluate(w2.test, verbose=1)[1]\n",
    "    \n",
    "    \n",
    "    np.save('./window_sensitivity/mae_window_' + \"{:02}\".format(window_width) + '.npy', performance['LSTM'])\n",
    "    \n",
    "    print(window_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf1624f-439f-4bbb-b040-f5d24598b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [np.load('./window_sensitivity/mae_window_' + time + '.npy') for time in [\"{:02}\".format(n) for n in range(1,25)]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81909a4-6d03-41f3-afa2-34ec682275b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = [n for n in range(0,24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3966d4ab-346c-4823-b1b1-2ec2a39c8d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694da6c-bd89-4c55-9abe-cc8b8984bb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
